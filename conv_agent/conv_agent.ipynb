{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ROBIN:\n",
    "    def __init__(self):\n",
    "        self.conversion_orchestrator = ConversationOrchestrator()\n",
    "        self.context_agent = ContextAgent()\n",
    "        self.collaborative_agent = CollaborativeAgent()\n",
    "        self.responder_agent = ResponderAgent()\n",
    "        self.follow_up_agent = FollowUpAgent()\n",
    "\n",
    "    def generate_reply(self, message):\n",
    "        if self.conversion_orchestrator.need_investigation(message):\n",
    "            if self.conversion_orchestrator.can_automate_context(message):\n",
    "                return self.process_with_context(message)\n",
    "            else:\n",
    "                return self.process_without_context(message)\n",
    "        else:\n",
    "            return self.responder_agent.generate_response(message)\n",
    "\n",
    "    def process_with_context(self, message):\n",
    "        context = self.context_agent.extract_context(message)\n",
    "        response = self.collaborative_agent.generate_response(message, context)\n",
    "        follow_up = self.follow_up_agent.generate_follow_up(response)\n",
    "        return response, follow_up\n",
    "\n",
    "\n",
    "    def process_without_context(self, message):\n",
    "        response = self.collaborative_agent.generate_response(message)\n",
    "        follow_up = self.follow_up_agent.generate_follow_up(response)\n",
    "        return response, follow_up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationOrchestrator:\n",
    "    def need_investigation(self, message):\n",
    "        complexity_score = self.analyze_complexity(message)\n",
    "        return complexity_score > 0.7\n",
    "    \n",
    "    def can_automate_context(self, message):\n",
    "        context_clues = self.extract_context_clues(message)\n",
    "        return len(context_clues) > 0\n",
    "    \n",
    "    def analyze_complexity(self, message):\n",
    "        length_factor = len(message) / 100  # Normalize length on a scale (assuming avg. length of 100 characters)\n",
    "        technical_terms = ['API', 'framework', 'ML model', 'database', 'server', 'algorithm']  # Add relevant terms\n",
    "        technical_count = sum(1 for term in technical_terms if term.lower() in message.lower())\n",
    "        \n",
    "        complexity_score = min(1.0, 0.5 * length_factor + 0.5 * (technical_count / len(technical_terms)))\n",
    "        return complexity_score\n",
    "\n",
    "    def extract_context_clues(self, message):\n",
    "        keywords = re.findall(r'\\b(database|API|framework|model|testing|deployment|response|user)\\b', message.lower())\n",
    "        code_snippets = re.findall(r'`.*?`', message)  # Matches anything within backticks, common for code snippets\n",
    "        specific_topics = re.findall(r'@[a-zA-Z]+', message)  # E.g., mentions like \"@CollaborativeAgent\"\n",
    "        \n",
    "        # Combine results for context clues\n",
    "        context_clues = keywords + code_snippets + specific_topics\n",
    "        return context_clues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextAgent:\n",
    "    def __init__(self):\n",
    "        self.code_context = []\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def extract_context(self, message):\n",
    "        code_snippets = self.extract_code_snippets(message)\n",
    "        self.update_code_context(code_snippets)\n",
    "        self.update_conversation_history(message)\n",
    "        return {\n",
    "            'code_context': self.code_context,\n",
    "            'conversation_history': self.conversation_history\n",
    "        }\n",
    "\n",
    "    def extract_code_snippets(self, message):\n",
    "        # Extract code snippets that are typically enclosed in backticks or identified by common syntax elements\n",
    "        code_snippets = re.findall(r'`([^`]*)`', message)  # Matches text within backticks\n",
    "        # Alternatively, you could also look for common code patterns (e.g., keywords or function-like structures)\n",
    "        code_snippets.extend(re.findall(r'\\b(def|class|return|import|from|if|else|for|while)\\b[^\\n]*', message))\n",
    "        return code_snippets\n",
    "\n",
    "    def update_code_context(self, code_snippets):\n",
    "        self.code_context.extend(code_snippets)\n",
    "        # Maintain a limited context size of the last 5 snippets\n",
    "        self.code_context = self.code_context[-5:]\n",
    "\n",
    "    def update_conversation_history(self, message):\n",
    "        self.conversation_history.append(message)\n",
    "        # Maintain a limited history size of the last 10 messages\n",
    "        self.conversation_history = self.conversation_history[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollaborativeAgent:\n",
    "    def __init__(self):\n",
    "        self.knowledge_base = KnowledgeBase()\n",
    "        self.nlp_processor = NLPProcessor()\n",
    "\n",
    "    def generate_response(self, message, context=None):\n",
    "        # Query knowledge base based on context, if provided\n",
    "        relevant_info = self.knowledge_base.query(context if context else message)\n",
    "        processed_message = self.nlp_processor.process(message, context)\n",
    "        \n",
    "        # Compose a final response using both processed message and relevant info\n",
    "        response = self.compose_response(processed_message, relevant_info)\n",
    "        return response\n",
    "\n",
    "    def compose_response(self, processed_message, relevant_info):\n",
    "        # Combine processed message with relevant information for a coherent response\n",
    "        if relevant_info:\n",
    "            # Integrate relevant info into response\n",
    "            response = f\"{processed_message} Here's what I found: {relevant_info}.\"\n",
    "        else:\n",
    "            # Provide processed message directly if no relevant info\n",
    "            response = processed_message\n",
    "        return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeBase:\n",
    "    def __init__(self):\n",
    "        self.data = {\n",
    "            \"machine learning\": \"Machine learning involves training algorithms to learn from data.\",\n",
    "            \"API\": \"An API allows communication between different software applications.\",\n",
    "            # Add more relevant info as key-value pairs\n",
    "        }\n",
    "\n",
    "    def query(self, keyword):\n",
    "        # Perform a simple keyword-based lookup\n",
    "        return self.data.get(keyword.lower(), \"I don't have information on that topic.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
      "     ----- ---------------------------------- 1.8/12.8 MB 10.1 MB/s eta 0:00:02\n",
      "     ----------- ---------------------------- 3.7/12.8 MB 9.1 MB/s eta 0:00:02\n",
      "     ------------------ --------------------- 5.8/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 7.9/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 9.2/12.8 MB 8.8 MB/s eta 0:00:01\n",
      "     -------------------------------- ------- 10.5/12.8 MB 9.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 12.3/12.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 8.5 MB/s eta 0:00:00\n",
      "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'tokens': ['ROBIN', 'can', 'help', 'developers', 'with', 'Python', 'coding', '.'], 'pos_tags': ['NOUN', 'AUX', 'VERB', 'NOUN', 'ADP', 'PROPN', 'NOUN', 'PUNCT'], 'entities': []}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "class NLPProcessor:\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "    def process(self, message):\n",
    "        doc = self.nlp(message)\n",
    "        tokens = [token.text for token in doc]\n",
    "        pos_tags = [token.pos_ for token in doc]\n",
    "        entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "        return {\n",
    "            'tokens': tokens,\n",
    "            'pos_tags': pos_tags,\n",
    "            'entities': entities\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "nlp_processor = NLPProcessor()\n",
    "result = nlp_processor.process(\"ROBIN can help developers with Python coding.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import re\n",
    "\n",
    "class ResponderAgent:\n",
    "    def __init__(self):\n",
    "        self.quick_responses = {\n",
    "            'greetings': ['Hello!', 'Hi there!', 'Greetings!'],\n",
    "            'farewells': ['Goodbye!', 'See you later!', 'Take care!'],\n",
    "            'thanks': ['You\\'re welcome!', 'Glad I could help!', 'My pleasure!']\n",
    "        }\n",
    "\n",
    "    def generate_response(self, message):\n",
    "        intent = self.classify_intent(message)\n",
    "        if intent in self.quick_responses:\n",
    "            return random.choice(self.quick_responses[intent])\n",
    "        else:\n",
    "            return self.generate_custom_response(message)\n",
    "\n",
    "    def classify_intent(self, message):\n",
    "        # Basic intent classification based on keyword matching\n",
    "        message = message.lower()\n",
    "        if re.search(r'\\b(hi|hello|hey)\\b', message):\n",
    "            return 'greetings'\n",
    "        elif re.search(r'\\b(goodbye|bye|see you|later)\\b', message):\n",
    "            return 'farewells'\n",
    "        elif re.search(r'\\b(thank you|thanks|appreciate)\\b', message):\n",
    "            return 'thanks'\n",
    "        else:\n",
    "            return 'custom'\n",
    "\n",
    "    def generate_custom_response(self, message):\n",
    "        # Example custom response handling based on message analysis\n",
    "        if \"help\" in message.lower():\n",
    "            return \"I'm here to assist you. Could you provide more details on what you need help with?\"\n",
    "        elif \"problem\" in message.lower():\n",
    "            return \"I'm sorry to hear that you're facing a problem. Let me know more about it so I can assist.\"\n",
    "        else:\n",
    "            return \"I'm here to help! Could you please clarify your request?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FollowUpAgent:\n",
    "    def __init__(self):\n",
    "        self.topic_analyzer = TopicAnalyzer()\n",
    "\n",
    "    def generate_follow_up(self, response):\n",
    "        topics = self.topic_analyzer.extract_topics(response)\n",
    "        follow_up_questions = self.create_follow_up_questions(topics)\n",
    "        return random.choice(follow_up_questions) if follow_up_questions else None\n",
    "\n",
    "    def create_follow_up_questions(self, topics):\n",
    "        questions = []\n",
    "        for topic in topics:\n",
    "            if topic == 'code':\n",
    "                questions.append(\"Would you like me to explain any part of the code in more detail?\")\n",
    "            elif topic == 'concept':\n",
    "                questions.append(\"Is there a specific aspect of this concept you'd like to explore further?\")\n",
    "            elif topic == 'error':\n",
    "                questions.append(\"Have you encountered any specific errors you'd like help with?\")\n",
    "        return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopicAnalyzer:\n",
    "    def extract_topics(self, response):\n",
    "        # Basic keyword-based topic extraction\n",
    "        topics = []\n",
    "        response = response.lower()\n",
    "        \n",
    "        if \"code\" in response or \"function\" in response:\n",
    "            topics.append(\"code\")\n",
    "        if \"concept\" in response or \"theory\" in response:\n",
    "            topics.append(\"concept\")\n",
    "        if \"error\" in response or \"bug\" in response:\n",
    "            topics.append(\"error\")\n",
    "        \n",
    "        # Add more topic detection logic as needed, e.g., \"performance,\" \"architecture\"\n",
    "        return topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def hello_world():\n",
      "    print('Hello, World!')\n",
      "for i in range(5):\n",
      "    print(i)\n"
     ]
    }
   ],
   "source": [
    "class CodeContextManager:\n",
    "    def __init__(self):\n",
    "        self.code_snippets = []\n",
    "        self.max_snippets = 5\n",
    "\n",
    "    def add_snippet(self, snippet):\n",
    "        self.code_snippets.append(snippet)\n",
    "        if len(self.code_snippets) > self.max_snippets:\n",
    "            self.code_snippets.pop(0)\n",
    "\n",
    "    def get_context(self):\n",
    "        return '\\n'.join(self.code_snippets)\n",
    "\n",
    "    def clear_context(self):\n",
    "        self.code_snippets.clear()\n",
    "\n",
    "# Usage example\n",
    "code_manager = CodeContextManager()\n",
    "code_manager.add_snippet(\"def hello_world():\\n    print('Hello, World!')\")\n",
    "code_manager.add_snippet(\"for i in range(5):\\n    print(i)\")\n",
    "print(code_manager.get_context())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\users\\aagam\\desktop\\pythonrev\\.venv\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\aagam\\desktop\\pythonrev\\.venv\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\aagam\\desktop\\pythonrev\\.venv\\lib\\site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\aagam\\desktop\\pythonrev\\.venv\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\aagam\\desktop\\pythonrev\\.venv\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "question\n",
      "farewell\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "class IntentClassifier:\n",
    "    def __init__(self):\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.classifier = MultinomialNB()\n",
    "        self.intents = ['question', 'code_help', 'explanation', 'greeting', 'farewell']\n",
    "\n",
    "    def train(self, training_data, labels):\n",
    "        X = self.vectorizer.fit_transform(training_data)\n",
    "        self.classifier.fit(X, labels)\n",
    "\n",
    "    def predict(self, message):\n",
    "        X = self.vectorizer.transform([message])\n",
    "        intent_index = self.classifier.predict(X)[0]\n",
    "        return self.intents[intent_index]\n",
    "\n",
    "# Usage example\n",
    "classifier = IntentClassifier()\n",
    "training_data = [\"How do I use ROBIN?\", \"def hello_world():\", \"Can you explain generators?\", \"Hello ROBIN\", \"Goodbye\"]\n",
    "labels = [0, 1, 2, 3, 4]\n",
    "classifier.train(training_data, labels)\n",
    "print(classifier.predict(\"What is the purpose??\"))\n",
    "print(classifier.predict(\"Goodbye\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mysql-connector-python\n",
      "  Downloading mysql_connector_python-9.1.0-cp311-cp311-win_amd64.whl.metadata (6.2 kB)\n",
      "Downloading mysql_connector_python-9.1.0-cp311-cp311-win_amd64.whl (16.1 MB)\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/16.1 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/16.1 MB 1.5 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 0.8/16.1 MB 1.6 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.3/16.1 MB 1.8 MB/s eta 0:00:09\n",
      "   --- ------------------------------------ 1.6/16.1 MB 1.7 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.1/16.1 MB 1.7 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 2.4/16.1 MB 1.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 2.6/16.1 MB 1.7 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 3.1/16.1 MB 1.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 3.7/16.1 MB 1.8 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 3.9/16.1 MB 1.8 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 4.5/16.1 MB 1.9 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 5.0/16.1 MB 1.9 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 5.5/16.1 MB 1.9 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 6.0/16.1 MB 2.0 MB/s eta 0:00:06\n",
      "   ---------------- ----------------------- 6.6/16.1 MB 2.0 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 7.1/16.1 MB 2.0 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 7.6/16.1 MB 2.0 MB/s eta 0:00:05\n",
      "   -------------------- ------------------- 8.1/16.1 MB 2.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 8.4/16.1 MB 2.1 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 9.2/16.1 MB 2.1 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 9.7/16.1 MB 2.1 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 10.2/16.1 MB 2.1 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 10.7/16.1 MB 2.2 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 11.3/16.1 MB 2.2 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 11.8/16.1 MB 2.2 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 12.6/16.1 MB 2.2 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 13.1/16.1 MB 2.2 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 13.6/16.1 MB 2.3 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 14.2/16.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 14.7/16.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 15.2/16.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  15.7/16.1 MB 2.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 16.1/16.1 MB 2.3 MB/s eta 0:00:00\n",
      "Installing collected packages: mysql-connector-python\n",
      "Successfully installed mysql-connector-python-9.1.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install mysql-connector-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Python is a high-level, interpreted programming language.',)]\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "\n",
    "class KnowledgeBase:\n",
    "    def __init__(self, host='localhost', port=3306, user='root', password='root', database='knowledge_base'):\n",
    "        self.conn = mysql.connector.connect(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            user=user,\n",
    "            password=password,\n",
    "            database=database\n",
    "        )\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self.create_table()\n",
    "\n",
    "    def create_table(self):\n",
    "        # Create the knowledge table if it doesn't exist\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS knowledge (\n",
    "                id INT AUTO_INCREMENT PRIMARY KEY,\n",
    "                topic VARCHAR(255) NOT NULL,\n",
    "                content TEXT NOT NULL\n",
    "            )\n",
    "        ''')\n",
    "        self.conn.commit()\n",
    "\n",
    "    def add_knowledge(self, topic, content):\n",
    "        self.cursor.execute('INSERT INTO knowledge (topic, content) VALUES (%s, %s)', (topic, content))\n",
    "        self.conn.commit()\n",
    "\n",
    "    def query(self, topic):\n",
    "        self.cursor.execute('SELECT content FROM knowledge WHERE topic LIKE %s', ('%' + topic + '%',))\n",
    "        return self.cursor.fetchall()\n",
    "\n",
    "    def close(self):\n",
    "        self.conn.close()\n",
    "\n",
    "# Usage example\n",
    "kb = KnowledgeBase()  # Replace with your actual password\n",
    "kb.add_knowledge('Python', 'Python is a high-level, interpreted programming language.')\n",
    "print(kb.query('Python'))\n",
    "kb.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Nested loop detected. Consider using list comprehension or generator.']\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "class RefactoringAssistant:\n",
    "    def analyze_code(self, code):\n",
    "        tree = ast.parse(code)\n",
    "        issues = []\n",
    "        for node in ast.walk(tree):\n",
    "            if isinstance(node, ast.FunctionDef):\n",
    "                if len(node.body) > 10:\n",
    "                    issues.append(f\"Function '{node.name}' is too long. Consider breaking it down.\")\n",
    "            elif isinstance(node, ast.For):\n",
    "                if isinstance(node.body[0], ast.For):\n",
    "                    issues.append(\"Nested loop detected. Consider using list comprehension or generator.\")\n",
    "        return issues\n",
    "\n",
    "# Usage example\n",
    "assistant = RefactoringAssistant()\n",
    "code = \"\"\"\n",
    "def complex_function():\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            print(i, j)\n",
    "    # ... more code ...\n",
    "\"\"\"\n",
    "print(assistant.analyze_code(code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test_a_is_int', 'test_b_is_int']\n"
     ]
    }
   ],
   "source": [
    "import inspect\n",
    "import ast\n",
    "\n",
    "class TestCaseGenerator:\n",
    "    def generate_test_cases(self, func):\n",
    "        signature = inspect.signature(func)\n",
    "        docstring = ast.get_docstring(ast.parse(inspect.getsource(func)))\n",
    "        \n",
    "        test_cases = []\n",
    "        for param in signature.parameters.values():\n",
    "            if param.annotation != inspect.Parameter.empty:\n",
    "                test_cases.append(self.generate_test_for_param(param))\n",
    "        \n",
    "        if docstring:\n",
    "            test_cases.extend(self.extract_examples_from_docstring(docstring))\n",
    "        \n",
    "        return test_cases\n",
    "\n",
    "    def generate_test_for_param(self, param):\n",
    "        if param.annotation == int:\n",
    "            return f\"test_{param.name}_is_int\"\n",
    "        elif param.annotation == str:\n",
    "            return f\"test_{param.name}_is_str\"\n",
    "        # Add more type checks as needed\n",
    "\n",
    "    def extract_examples_from_docstring(self, docstring):\n",
    "        example_tests = []\n",
    "        examples = re.findall(r\">>> (.+)\", docstring)  # Matches lines starting with \">>>\"\n",
    "        \n",
    "        for example in examples:\n",
    "            example_tests.append(f\"example_test_{example.replace(' ', '_')}\")\n",
    "        \n",
    "        return example_tests\n",
    "\n",
    "# Usage example\n",
    "def add_numbers(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Add two numbers.\n",
    "    \n",
    "    Examples:\n",
    "    >>> add_numbers(1, 2)\n",
    "    3\n",
    "    >>> add_numbers(-1, 1)\n",
    "    0\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "generator = TestCaseGenerator()\n",
    "print(generator.generate_test_cases(add_numbers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
